{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"A100","authorship_tag":"ABX9TyP1QoRh9/Nn3y6rDC5kDaFp"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["%pip install transformers\n","%pip install torch\n","%pip install pandas\n","%pip install datasets"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uE7eH_Z1bemE","executionInfo":{"status":"ok","timestamp":1720192131923,"user_tz":-420,"elapsed":21234,"user":{"displayName":"Tuấn Nguyễn Duy","userId":"13781921621114420072"}},"outputId":"891abf20-5b8a-4685-e43a-3c852f1374e5"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.41.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.4)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.4)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n","Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.6.2)\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.0+cu121)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.15.4)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n","Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch) (8.9.2.26)\n","Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.3.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch) (11.0.2.54)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch) (10.3.2.106)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch) (11.4.5.107)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.0.106)\n","Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch) (2.20.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n","Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.3.0)\n","Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.5.82)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n","Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.0.3)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.4)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n","Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.25.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n","Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.20.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.15.4)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n","Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (16.1.0)\n","Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n","Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n","Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n","Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.4)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n","Requirement already satisfied: fsspec[http]<=2024.5.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n","Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.23.4)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.6.2)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"]}]},{"cell_type":"code","execution_count":2,"metadata":{"id":"EsEer0f5avSz","executionInfo":{"status":"ok","timestamp":1720192140141,"user_tz":-420,"elapsed":5278,"user":{"displayName":"Tuấn Nguyễn Duy","userId":"13781921621114420072"}}},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader, random_split\n","from torch.utils.tensorboard import SummaryWriter\n","\n","# Math\n","import math\n","\n","# HuggingFace libraries\n","from datasets import load_dataset\n","from tokenizers import Tokenizer\n","from tokenizers.models import WordLevel\n","from tokenizers.trainers import WordLevelTrainer\n","from tokenizers.pre_tokenizers import Whitespace\n","\n","# Pathlib\n","from pathlib import Path\n","\n","# typing\n","from typing import Any\n","\n","# Library for progress bars in loops\n","from tqdm import tqdm\n","\n","# Importing library of warnings\n","import warnings"]},{"cell_type":"markdown","source":["**Input Embeddings**"],"metadata":{"id":"0OS2I7cCcbCS"}},{"cell_type":"code","source":["# Creating Input Embeddings\n","class InputEmbeddings(nn.Module):\n","\n","    def __init__(self, d_model: int, vocab_size: int):\n","        super().__init__()\n","        self.d_model = d_model # Dimension of vectors (512)\n","        self.vocab_size = vocab_size # Size of the vocabulary\n","        self.embedding = nn.Embedding(vocab_size, d_model) # PyTorch layer that converts integer indices to dense embeddings\n","\n","    def forward(self, x):\n","        return self.embedding(x) * math.sqrt(self.d_model) # Normalizing the variance of the embeddings"],"metadata":{"id":"rjPlH05Qa_FM","executionInfo":{"status":"ok","timestamp":1720192178520,"user_tz":-420,"elapsed":1,"user":{"displayName":"Tuấn Nguyễn Duy","userId":"13781921621114420072"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["**Positional Encoding**"],"metadata":{"id":"ySG7awYJcfcV"}},{"cell_type":"code","source":["# Creating the Positional Encoding\n","class PositionalEncoding(nn.Module):\n","\n","    def __init__(self, d_model: int, seq_len: int, dropout: float) -> None:\n","        super().__init__()\n","        self.d_model = d_model # Dimensionality of the model\n","        self.seq_len = seq_len # Maximum sequence length\n","        self.dropout = nn.Dropout(dropout) # Dropout layer to prevent overfitting\n","\n","        # Creating a positional encoding matrix of shape (seq_len, d_model) filled with zeros\n","        pe = torch.zeros(seq_len, d_model)\n","\n","        # Creating a tensor representing positions (0 to seq_len - 1)\n","        position = torch.arange(0, seq_len, dtype = torch.float).unsqueeze(1) # Transforming 'position' into a 2D tensor['seq_len, 1']\n","\n","        # Creating the division term for the positional encoding formula\n","        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n","\n","        # Apply sine to even indices in pe\n","        pe[:, 0::2] = torch.sin(position * div_term)\n","        # Apply cosine to odd indices in pe\n","        pe[:, 1::2] = torch.cos(position * div_term)\n","\n","        # Adding an extra dimension at the beginning of pe matrix for batch handling\n","        pe = pe.unsqueeze(0)\n","\n","        # Registering 'pe' as buffer. Buffer is a tensor not considered as a model parameter\n","        self.register_buffer('pe', pe)\n","\n","    def forward(self,x):\n","        # Addind positional encoding to the input tensor X\n","        x = x + (self.pe[:, :x.shape[1], :]).requires_grad_(False)\n","        return self.dropout(x) # Dropout for regularization"],"metadata":{"id":"pKdhuW5Nbd5M","executionInfo":{"status":"ok","timestamp":1720192218040,"user_tz":-420,"elapsed":567,"user":{"displayName":"Tuấn Nguyễn Duy","userId":"13781921621114420072"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["**Layer Normalization**"],"metadata":{"id":"VOcr9IZqclcN"}},{"cell_type":"code","source":["# Creating Layer Normalization\n","class LayerNormalization(nn.Module):\n","\n","    def __init__(self, eps: float = 10**-6) -> None: # We define epsilon as 0.000001 to avoid division by zero\n","        super().__init__()\n","        self.eps = eps\n","\n","        # We define alpha as a trainable parameter and initialize it with ones\n","        self.alpha = nn.Parameter(torch.ones(1)) # One-dimensional tensor that will be used to scale the input data\n","\n","        # We define bias as a trainable parameter and initialize it with zeros\n","        self.bias = nn.Parameter(torch.zeros(1)) # One-dimensional tenso that will be added to the input data\n","\n","    def forward(self, x):\n","        mean = x.mean(dim = -1, keepdim = True) # Computing the mean of the input data. Keeping the number of dimensions unchanged\n","        std = x.std(dim = -1, keepdim = True) # Computing the standard deviation of the input data. Keeping the number of dimensions unchanged\n","\n","        # Returning the normalized input\n","        return self.alpha * (x-mean) / (std + self.eps) + self.bias"],"metadata":{"id":"xQ-XzF2sbd6K","executionInfo":{"status":"ok","timestamp":1720192246501,"user_tz":-420,"elapsed":883,"user":{"displayName":"Tuấn Nguyễn Duy","userId":"13781921621114420072"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["**Feed-Forward Network**"],"metadata":{"id":"IOXWHs3Mcr5U"}},{"cell_type":"code","source":["# Creating Feed Forward Layers\n","class FeedForwardBlock(nn.Module):\n","\n","    def __init__(self, d_model: int, d_ff: int, dropout: float) -> None:\n","        super().__init__()\n","        # First linear transformation\n","        self.linear_1 = nn.Linear(d_model, d_ff) # W1 & b1\n","        self.dropout = nn.Dropout(dropout) # Dropout to prevent overfitting\n","        # Second linear transformation\n","        self.linear_2 = nn.Linear(d_ff, d_model) # W2 & b2\n","\n","    def forward(self, x):\n","        # (Batch, seq_len, d_model) --> (batch, seq_len, d_ff) -->(batch, seq_len, d_model)\n","        return self.linear_2(self.dropout(torch.relu(self.linear_1(x))))"],"metadata":{"id":"bBPBLruecplF","executionInfo":{"status":"ok","timestamp":1720192265797,"user_tz":-420,"elapsed":1283,"user":{"displayName":"Tuấn Nguyễn Duy","userId":"13781921621114420072"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["**Multi-Head Attention**"],"metadata":{"id":"AJnrW2qbcwfj"}},{"cell_type":"code","source":["# Creating the Multi-Head Attention block\n","class MultiHeadAttentionBlock(nn.Module):\n","\n","    def __init__(self, d_model: int, h: int, dropout: float) -> None: # h = number of heads\n","        super().__init__()\n","        self.d_model = d_model\n","        self.h = h\n","\n","        # We ensure that the dimensions of the model is divisible by the number of heads\n","        assert d_model % h == 0, 'd_model is not divisible by h'\n","\n","        # d_k is the dimension of each attention head's key, query, and value vectors\n","        self.d_k = d_model // h # d_k formula, like in the original \"Attention Is All You Need\" paper\n","\n","        # Defining the weight matrices\n","        self.w_q = nn.Linear(d_model, d_model) # W_q\n","        self.w_k = nn.Linear(d_model, d_model) # W_k\n","        self.w_v = nn.Linear(d_model, d_model) # W_v\n","        self.w_o = nn.Linear(d_model, d_model) # W_o\n","\n","        self.dropout = nn.Dropout(dropout) # Dropout layer to avoid overfitting\n","\n","\n","    @staticmethod\n","    def attention(query, key, value, mask, dropout: nn.Dropout):# mask => When we want certain words to NOT interact with others, we \"hide\" them\n","\n","        d_k = query.shape[-1] # The last dimension of query, key, and value\n","\n","        # We calculate the Attention(Q,K,V) as in the formula in the image above\n","        attention_scores = (query @ key.transpose(-2,-1)) / math.sqrt(d_k) # @ = Matrix multiplication sign in PyTorch\n","\n","        # Before applying the softmax, we apply the mask to hide some interactions between words\n","        if mask is not None: # If a mask IS defined...\n","            attention_scores.masked_fill_(mask == 0, -1e9) # Replace each value where mask is equal to 0 by -1e9\n","        attention_scores = attention_scores.softmax(dim = -1) # Applying softmax\n","        if dropout is not None: # If a dropout IS defined...\n","            attention_scores = dropout(attention_scores) # We apply dropout to prevent overfitting\n","\n","        return (attention_scores @ value), attention_scores # Multiply the output matrix by the V matrix, as in the formula\n","\n","    def forward(self, q, k, v, mask):\n","\n","        query = self.w_q(q) # Q' matrix\n","        key = self.w_k(k) # K' matrix\n","        value = self.w_v(v) # V' matrix\n","\n","\n","        # Splitting results into smaller matrices for the different heads\n","        # Splitting embeddings (third dimension) into h parts\n","        query = query.view(query.shape[0], query.shape[1], self.h, self.d_k).transpose(1,2) # Transpose => bring the head to the second dimension\n","        key = key.view(key.shape[0], key.shape[1], self.h, self.d_k).transpose(1,2) # Transpose => bring the head to the second dimension\n","        value = value.view(value.shape[0], value.shape[1], self.h, self.d_k).transpose(1,2) # Transpose => bring the head to the second dimension\n","\n","        # Obtaining the output and the attention scores\n","        x, self.attention_scores = MultiHeadAttentionBlock.attention(query, key, value, mask, self.dropout)\n","\n","        # Obtaining the H matrix\n","        x = x.transpose(1, 2).contiguous().view(x.shape[0], -1, self.h * self.d_k)\n","\n","        return self.w_o(x) # Multiply the H matrix by the weight matrix W_o, resulting in the MH-A matrix"],"metadata":{"id":"d5hhykhRcuZ3","executionInfo":{"status":"ok","timestamp":1720192294929,"user_tz":-420,"elapsed":928,"user":{"displayName":"Tuấn Nguyễn Duy","userId":"13781921621114420072"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["**Residual Connection**"],"metadata":{"id":"cG11z9Q9c20q"}},{"cell_type":"code","source":["# Building Residual Connection\n","class ResidualConnection(nn.Module):\n","    def __init__(self, dropout: float) -> None:\n","        super().__init__()\n","        self.dropout = nn.Dropout(dropout) # We use a dropout layer to prevent overfitting\n","        self.norm = LayerNormalization() # We use a normalization layer\n","\n","    def forward(self, x, sublayer):\n","        # We normalize the input and add it to the original input 'x'. This creates the residual connection process.\n","        return x + self.dropout(sublayer(self.norm(x)))"],"metadata":{"id":"9HPbzqegc1B1","executionInfo":{"status":"ok","timestamp":1720192311928,"user_tz":-420,"elapsed":1488,"user":{"displayName":"Tuấn Nguyễn Duy","userId":"13781921621114420072"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["**Encoder**"],"metadata":{"id":"rpOsWYlhc7gA"}},{"cell_type":"code","source":["# Building Encoder Block\n","class EncoderBlock(nn.Module):\n","\n","    # This block takes in the MultiHeadAttentionBlock and FeedForwardBlock, as well as the dropout rate for the residual connections\n","    def __init__(self, self_attention_block: MultiHeadAttentionBlock, feed_forward_block: FeedForwardBlock, dropout: float) -> None:\n","        super().__init__()\n","        # Storing the self-attention block and feed-forward block\n","        self.self_attention_block = self_attention_block\n","        self.feed_forward_block = feed_forward_block\n","        self.residual_connections = nn.ModuleList([ResidualConnection(dropout) for _ in range(2)]) # 2 Residual Connections with dropout\n","\n","    def forward(self, x, src_mask):\n","        # Applying the first residual connection with the self-attention block\n","        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, src_mask)) # Three 'x's corresponding to query, key, and value inputs plus source mask\n","\n","        # Applying the second residual connection with the feed-forward block\n","        x = self.residual_connections[1](x, self.feed_forward_block)\n","        return x # Output tensor after applying self-attention and feed-forward layers with residual connections."],"metadata":{"id":"a832yI_Nc5G2","executionInfo":{"status":"ok","timestamp":1720192331505,"user_tz":-420,"elapsed":541,"user":{"displayName":"Tuấn Nguyễn Duy","userId":"13781921621114420072"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["# Building Encoder\n","# An Encoder can have several Encoder Blocks\n","class Encoder(nn.Module):\n","\n","    # The Encoder takes in instances of 'EncoderBlock'\n","    def __init__(self, layers: nn.ModuleList) -> None:\n","        super().__init__()\n","        self.layers = layers # Storing the EncoderBlocks\n","        self.norm = LayerNormalization() # Layer for the normalization of the output of the encoder layers\n","\n","    def forward(self, x, mask):\n","        # Iterating over each EncoderBlock stored in self.layers\n","        for layer in self.layers:\n","            x = layer(x, mask) # Applying each EncoderBlock to the input tensor 'x'\n","        return self.norm(x) # Normalizing output\n"],"metadata":{"id":"wcvlyJAOc-ot","executionInfo":{"status":"ok","timestamp":1720192338605,"user_tz":-420,"elapsed":564,"user":{"displayName":"Tuấn Nguyễn Duy","userId":"13781921621114420072"}}},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":["**Decoder**"],"metadata":{"id":"JqA8fOjldBeb"}},{"cell_type":"code","source":["# Building Decoder Block\n","class DecoderBlock(nn.Module):\n","\n","    # The DecoderBlock takes in two MultiHeadAttentionBlock. One is self-attention, while the other is cross-attention.\n","    # It also takes in the feed-forward block and the dropout rate\n","    def __init__(self,  self_attention_block: MultiHeadAttentionBlock, cross_attention_block: MultiHeadAttentionBlock, feed_forward_block: FeedForwardBlock, dropout: float) -> None:\n","        super().__init__()\n","        self.self_attention_block = self_attention_block\n","        self.cross_attention_block = cross_attention_block\n","        self.feed_forward_block = feed_forward_block\n","        self.residual_connections = nn.ModuleList([ResidualConnection(dropout) for _ in range(3)]) # List of three Residual Connections with dropout rate\n","\n","    def forward(self, x, encoder_output, src_mask, tgt_mask):\n","\n","        # Self-Attention block with query, key, and value plus the target language mask\n","        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, tgt_mask))\n","\n","        # The Cross-Attention block using two 'encoder_ouput's for key and value plus the source language mask. It also takes in 'x' for Decoder queries\n","        x = self.residual_connections[1](x, lambda x: self.cross_attention_block(x, encoder_output, encoder_output, src_mask))\n","\n","        # Feed-forward block with residual connections\n","        x = self.residual_connections[2](x, self.feed_forward_block)\n","        return x"],"metadata":{"id":"BKQ8VJ79dAWW","executionInfo":{"status":"ok","timestamp":1720192372876,"user_tz":-420,"elapsed":1448,"user":{"displayName":"Tuấn Nguyễn Duy","userId":"13781921621114420072"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["# Building Decoder\n","# A Decoder can have several Decoder Blocks\n","class Decoder(nn.Module):\n","\n","    # The Decoder takes in instances of 'DecoderBlock'\n","    def __init__(self, layers: nn.ModuleList) -> None:\n","        super().__init__()\n","\n","        # Storing the 'DecoderBlock's\n","        self.layers = layers\n","        self.norm = LayerNormalization() # Layer to normalize the output\n","\n","    def forward(self, x, encoder_output, src_mask, tgt_mask):\n","\n","        # Iterating over each DecoderBlock stored in self.layers\n","        for layer in self.layers:\n","            # Applies each DecoderBlock to the input 'x' plus the encoder output and source and target masks\n","            x = layer(x, encoder_output, src_mask, tgt_mask)\n","        return self.norm(x) # Returns normalized output\n"],"metadata":{"id":"R1LelLYqdH6t","executionInfo":{"status":"ok","timestamp":1720192379996,"user_tz":-420,"elapsed":578,"user":{"displayName":"Tuấn Nguyễn Duy","userId":"13781921621114420072"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["# Buiding Linear Layer\n","class ProjectionLayer(nn.Module):\n","    def __init__(self, d_model: int, vocab_size: int) -> None: # Model dimension and the size of the output vocabulary\n","        super().__init__()\n","        self.proj = nn.Linear(d_model, vocab_size) # Linear layer for projecting the feature space of 'd_model' to the output space of 'vocab_size'\n","    def forward(self, x):\n","        return torch.log_softmax(self.proj(x), dim = -1) # Applying the log Softmax function to the output\n"],"metadata":{"id":"uafw-tuxdKVs","executionInfo":{"status":"ok","timestamp":1720192392757,"user_tz":-420,"elapsed":2,"user":{"displayName":"Tuấn Nguyễn Duy","userId":"13781921621114420072"}}},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":["**Building the Transformer**"],"metadata":{"id":"QaV0QVKFdObc"}},{"cell_type":"code","source":["# Creating the Transformer Architecture\n","class Transformer(nn.Module):\n","\n","    # This takes in the encoder and decoder, as well the embeddings for the source and target language.\n","    # It also takes in the Positional Encoding for the source and target language, as well as the projection layer\n","    def __init__(self, encoder: Encoder, decoder: Decoder, src_embed: InputEmbeddings, tgt_embed: InputEmbeddings, src_pos: PositionalEncoding, tgt_pos: PositionalEncoding, projection_layer: ProjectionLayer) -> None:\n","        super().__init__()\n","        self.encoder = encoder\n","        self.decoder = decoder\n","        self.src_embed = src_embed\n","        self.tgt_embed = tgt_embed\n","        self.src_pos = src_pos\n","        self.tgt_pos = tgt_pos\n","        self.projection_layer = projection_layer\n","\n","    # Encoder\n","    def encode(self, src, src_mask):\n","        src = self.src_embed(src) # Applying source embeddings to the input source language\n","        src = self.src_pos(src) # Applying source positional encoding to the source embeddings\n","        return self.encoder(src, src_mask) # Returning the source embeddings plus a source mask to prevent attention to certain elements\n","\n","    # Decoder\n","    def decode(self, encoder_output, src_mask, tgt, tgt_mask):\n","        tgt = self.tgt_embed(tgt) # Applying target embeddings to the input target language (tgt)\n","        tgt = self.tgt_pos(tgt) # Applying target positional encoding to the target embeddings\n","\n","        # Returning the target embeddings, the output of the encoder, and both source and target masks\n","        # The target mask ensures that the model won't 'see' future elements of the sequence\n","        return self.decoder(tgt, encoder_output, src_mask, tgt_mask)\n","\n","    # Applying Projection Layer with the Softmax function to the Decoder output\n","    def project(self, x):\n","        return self.projection_layer(x)\n"],"metadata":{"id":"SsM1e-CRdNro","executionInfo":{"status":"ok","timestamp":1720192444482,"user_tz":-420,"elapsed":1099,"user":{"displayName":"Tuấn Nguyễn Duy","userId":"13781921621114420072"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["# Building & Initializing Transformer\n","\n","# Definin function and its parameter, including model dimension, number of encoder and decoder stacks, heads, etc.\n","def build_transformer(src_vocab_size: int, tgt_vocab_size: int, src_seq_len: int, tgt_seq_len: int, d_model: int = 512, N: int = 6, h: int = 8, dropout: float = 0.1, d_ff: int = 2048) -> Transformer:\n","\n","    # Creating Embedding layers\n","    src_embed = InputEmbeddings(d_model, src_vocab_size) # Source language (Source Vocabulary to 512-dimensional vectors)\n","    tgt_embed = InputEmbeddings(d_model, tgt_vocab_size) # Target language (Target Vocabulary to 512-dimensional vectors)\n","\n","    # Creating Positional Encoding layers\n","    src_pos = PositionalEncoding(d_model, src_seq_len, dropout) # Positional encoding for the source language embeddings\n","    tgt_pos = PositionalEncoding(d_model, tgt_seq_len, dropout) # Positional encoding for the target language embeddings\n","\n","    # Creating EncoderBlocks\n","    encoder_blocks = [] # Initial list of empty EncoderBlocks\n","    for _ in range(N): # Iterating 'N' times to create 'N' EncoderBlocks (N = 6)\n","        encoder_self_attention_block = MultiHeadAttentionBlock(d_model, h, dropout) # Self-Attention\n","        feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout) # FeedForward\n","\n","        # Combine layers into an EncoderBlock\n","        encoder_block = EncoderBlock(encoder_self_attention_block, feed_forward_block, dropout)\n","        encoder_blocks.append(encoder_block) # Appending EncoderBlock to the list of EncoderBlocks\n","\n","    # Creating DecoderBlocks\n","    decoder_blocks = [] # Initial list of empty DecoderBlocks\n","    for _ in range(N): # Iterating 'N' times to create 'N' DecoderBlocks (N = 6)\n","        decoder_self_attention_block = MultiHeadAttentionBlock(d_model, h, dropout) # Self-Attention\n","        decoder_cross_attention_block = MultiHeadAttentionBlock(d_model, h, dropout) # Cross-Attention\n","        feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout) # FeedForward\n","\n","        # Combining layers into a DecoderBlock\n","        decoder_block = DecoderBlock(decoder_self_attention_block, decoder_cross_attention_block, feed_forward_block, dropout)\n","        decoder_blocks.append(decoder_block) # Appending DecoderBlock to the list of DecoderBlocks\n","\n","    # Creating the Encoder and Decoder by using the EncoderBlocks and DecoderBlocks lists\n","    encoder = Encoder(nn.ModuleList(encoder_blocks))\n","    decoder = Decoder(nn.ModuleList(decoder_blocks))\n","\n","    # Creating projection layer\n","    projection_layer = ProjectionLayer(d_model, tgt_vocab_size) # Map the output of Decoder to the Target Vocabulary Space\n","\n","    # Creating the transformer by combining everything above\n","    transformer = Transformer(encoder, decoder, src_embed, tgt_embed, src_pos, tgt_pos, projection_layer)\n","\n","    # Initialize the parameters\n","    for p in transformer.parameters():\n","        if p.dim() > 1:\n","            nn.init.xavier_uniform_(p)\n","\n","    return transformer # Assembled and initialized Transformer. Ready to be trained and validated!"],"metadata":{"id":"l81p0Wj7dZwq","executionInfo":{"status":"ok","timestamp":1720193062622,"user_tz":-420,"elapsed":605,"user":{"displayName":"Tuấn Nguyễn Duy","userId":"13781921621114420072"}}},"execution_count":53,"outputs":[]},{"cell_type":"markdown","source":["**Tokenizer**"],"metadata":{"id":"sEf3nnFYdieW"}},{"cell_type":"code","source":["# Defining Tokenizer\n","def build_tokenizer(config, ds, lang):\n","\n","    # Crating a file path for the tokenizer\n","    tokenizer_path = Path(config['tokenizer_file'].format(lang))\n","\n","    # Checking if Tokenizer already exists\n","    if not Path.exists(tokenizer_path):\n","\n","        # If it doesn't exist, we create a new one\n","        tokenizer = Tokenizer(WordLevel(unk_token = '[UNK]')) # Initializing a new world-level tokenizer\n","        tokenizer.pre_tokenizer = Whitespace() # We will split the text into tokens based on whitespace\n","\n","        # Creating a trainer for the new tokenizer\n","        trainer = WordLevelTrainer(special_tokens = [\"[UNK]\", \"[PAD]\",\n","                                                     \"[SOS]\", \"[EOS]\"], min_frequency = 2) # Defining Word Level strategy and special tokens\n","\n","        # Training new tokenizer on sentences from the dataset and language specified\n","        tokenizer.train_from_iterator(get_all_sentences(ds, lang), trainer = trainer)\n","        tokenizer.save(str(tokenizer_path)) # Saving trained tokenizer to the file path specified at the beginning of the function\n","    else:\n","        tokenizer = Tokenizer.from_file(str(tokenizer_path)) # If the tokenizer already exist, we load it\n","    return tokenizer # Returns the loaded tokenizer or the trained tokenizer"],"metadata":{"id":"bQ7GaZiZdao4","executionInfo":{"status":"ok","timestamp":1720193064222,"user_tz":-420,"elapsed":2,"user":{"displayName":"Tuấn Nguyễn Duy","userId":"13781921621114420072"}}},"execution_count":54,"outputs":[]},{"cell_type":"markdown","source":["**Loading Dataset**"],"metadata":{"id":"7aZQsX47dtj1"}},{"cell_type":"code","source":["def get_all_sentences(ds, lang):\n","    for pair in ds:\n","        yield pair['translation'][lang]"],"metadata":{"id":"PEy7kxmUdr5-","executionInfo":{"status":"ok","timestamp":1720193064223,"user_tz":-420,"elapsed":1,"user":{"displayName":"Tuấn Nguyễn Duy","userId":"13781921621114420072"}}},"execution_count":55,"outputs":[]},{"cell_type":"code","source":["def get_ds(config):\n","\n","    # Loading the train portion of the OpusBooks dataset.\n","    # The Language pairs will be defined in the 'config' dictionary we will build later\n","    ds_raw = load_dataset('opus_books', f'{config[\"lang_src\"]}-{config[\"lang_tgt\"]}', split = 'train')\n","\n","    # Building or loading tokenizer for both the source and target languages\n","    tokenizer_src = build_tokenizer(config, ds_raw, config['lang_src'])\n","    tokenizer_tgt = build_tokenizer(config, ds_raw, config['lang_tgt'])\n","\n","    # Splitting the dataset for training and validation\n","    train_ds_size = int(0.9 * len(ds_raw)) # 90% for training\n","    val_ds_size = len(ds_raw) - train_ds_size # 10% for validation\n","    train_ds_raw, val_ds_raw = random_split(ds_raw, [train_ds_size, val_ds_size]) # Randomly splitting the dataset\n","\n","    # Processing data with the BilingualDataset class, which we will define below\n","    train_ds = BilingualDataset(train_ds_raw, tokenizer_src, tokenizer_tgt, config['lang_src'], config['lang_tgt'], config['seq_len'])\n","    val_ds = BilingualDataset(val_ds_raw, tokenizer_src, tokenizer_tgt, config['lang_src'], config['lang_tgt'], config['seq_len'])\n","\n","    # Iterating over the entire dataset and printing the maximum length found in the sentences of both the source and target languages\n","    max_len_src = 0\n","    max_len_tgt = 0\n","    for pair in ds_raw:\n","        src_ids = tokenizer_src.encode(pair['translation'][config['lang_src']]).ids\n","        tgt_ids = tokenizer_src.encode(pair['translation'][config['lang_tgt']]).ids\n","        max_len_src = max(max_len_src, len(src_ids))\n","        max_len_tgt = max(max_len_tgt, len(tgt_ids))\n","\n","    print(f'Max length of source sentence: {max_len_src}')\n","    print(f'Max length of target sentence: {max_len_tgt}')\n","\n","    # Creating dataloaders for the training and validadion sets\n","    # Dataloaders are used to iterate over the dataset in batches during training and validation\n","    train_dataloader = DataLoader(train_ds, batch_size = config['batch_size'], shuffle = True) # Batch size will be defined in the config dictionary\n","    val_dataloader = DataLoader(val_ds, batch_size = 1, shuffle = True)\n","\n","    return train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt # Returning the DataLoader objects and tokenizers"],"metadata":{"id":"XwWN7ud8d0Tk","executionInfo":{"status":"ok","timestamp":1720193065228,"user_tz":-420,"elapsed":2,"user":{"displayName":"Tuấn Nguyễn Duy","userId":"13781921621114420072"}}},"execution_count":56,"outputs":[]},{"cell_type":"code","source":["def casual_mask(size):\n","        # Creating a square matrix of dimensions 'size x size' filled with ones\n","        mask = torch.triu(torch.ones(1, size, size), diagonal = 1).type(torch.int)\n","        return mask == 0"],"metadata":{"id":"iJogTsA1d7yr","executionInfo":{"status":"ok","timestamp":1720193065228,"user_tz":-420,"elapsed":2,"user":{"displayName":"Tuấn Nguyễn Duy","userId":"13781921621114420072"}}},"execution_count":57,"outputs":[]},{"cell_type":"code","source":["class BilingualDataset(Dataset):\n","\n","    # This takes in the dataset contaning sentence pairs, the tokenizers for target and source languages, and the strings of source and target languages\n","    # 'seq_len' defines the sequence length for both languages\n","    def __init__(self, ds, tokenizer_src, tokenizer_tgt, src_lang, tgt_lang, seq_len) -> None:\n","        super().__init__()\n","\n","        self.seq_len = seq_len\n","        self.ds = ds\n","        self.tokenizer_src = tokenizer_src\n","        self.tokenizer_tgt = tokenizer_tgt\n","        self.src_lang = src_lang\n","        self.tgt_lang = tgt_lang\n","\n","        # Defining special tokens by using the target language tokenizer\n","        self.sos_token = torch.tensor([tokenizer_tgt.token_to_id(\"[SOS]\")], dtype=torch.int64)\n","        self.eos_token = torch.tensor([tokenizer_tgt.token_to_id(\"[EOS]\")], dtype=torch.int64)\n","        self.pad_token = torch.tensor([tokenizer_tgt.token_to_id(\"[PAD]\")], dtype=torch.int64)\n","\n","\n","    # Total number of instances in the dataset (some pairs are larger than others)\n","    def __len__(self):\n","        return len(self.ds)\n","\n","    # Using the index to retrive source and target texts\n","    def __getitem__(self, index: Any) -> Any:\n","        src_target_pair = self.ds[index]\n","        src_text = src_target_pair['translation'][self.src_lang]\n","        tgt_text = src_target_pair['translation'][self.tgt_lang]\n","\n","        # Tokenizing source and target texts\n","        enc_input_tokens = self.tokenizer_src.encode(src_text).ids\n","        dec_input_tokens = self.tokenizer_tgt.encode(tgt_text).ids\n","\n","        # Computing how many padding tokens need to be added to the tokenized texts\n","        # Source tokens\n","        enc_num_padding_tokens = self.seq_len - len(enc_input_tokens) - 2 # Subtracting the two '[EOS]' and '[SOS]' special tokens\n","        # Target tokens\n","        dec_num_padding_tokens = self.seq_len - len(dec_input_tokens) - 1 # Subtracting the '[SOS]' special token\n","\n","        # If the texts exceed the 'seq_len' allowed, it will raise an error. This means that one of the sentences in the pair is too long to be processed\n","        # given the current sequence length limit (this will be defined in the config dictionary below)\n","        if enc_num_padding_tokens < 0 or dec_num_padding_tokens < 0:\n","            raise ValueError('Sentence is too long')\n","\n","        # Building the encoder input tensor by combining several elements\n","        encoder_input = torch.cat(\n","            [\n","            self.sos_token, # inserting the '[SOS]' token\n","            torch.tensor(enc_input_tokens, dtype = torch.int64), # Inserting the tokenized source text\n","            self.eos_token, # Inserting the '[EOS]' token\n","            torch.tensor([self.pad_token] * enc_num_padding_tokens, dtype = torch.int64) # Addind padding tokens\n","            ]\n","        )\n","\n","        # Building the decoder input tensor by combining several elements\n","        decoder_input = torch.cat(\n","            [\n","                self.sos_token, # inserting the '[SOS]' token\n","                torch.tensor(dec_input_tokens, dtype = torch.int64), # Inserting the tokenized target text\n","                torch.tensor([self.pad_token] * dec_num_padding_tokens, dtype = torch.int64) # Addind padding tokens\n","            ]\n","\n","        )\n","\n","        # Creating a label tensor, the expected output for training the model\n","        label = torch.cat(\n","            [\n","                torch.tensor(dec_input_tokens, dtype = torch.int64), # Inserting the tokenized target text\n","                self.eos_token, # Inserting the '[EOS]' token\n","                torch.tensor([self.pad_token] * dec_num_padding_tokens, dtype = torch.int64) # Adding padding tokens\n","\n","            ]\n","        )\n","\n","        # Ensuring that the length of each tensor above is equal to the defined 'seq_len'\n","        assert encoder_input.size(0) == self.seq_len\n","        assert decoder_input.size(0) == self.seq_len\n","        assert label.size(0) == self.seq_len\n","\n","        return {\n","            'encoder_input': encoder_input,\n","            'decoder_input': decoder_input,\n","            'encoder_mask': (encoder_input != self.pad_token).unsqueeze(0).unsqueeze(0).int(),\n","            'decoder_mask': (decoder_input != self.pad_token).unsqueeze(0).unsqueeze(0).int() & casual_mask(decoder_input.size(0)),\n","            'label': label,\n","            'src_text': src_text,\n","            'tgt_text': tgt_text\n","        }"],"metadata":{"id":"zmYCp1Lld8ih","executionInfo":{"status":"ok","timestamp":1720193065228,"user_tz":-420,"elapsed":1,"user":{"displayName":"Tuấn Nguyễn Duy","userId":"13781921621114420072"}}},"execution_count":58,"outputs":[]},{"cell_type":"markdown","source":["**Validation Loop**"],"metadata":{"id":"3pXR4z0WeC2H"}},{"cell_type":"code","source":["# Define function to obtain the most probable next token\n","def greedy_decode(model, source, source_mask, tokenizer_src, tokenizer_tgt, max_len, device):\n","    # Retrieving the indices from the start and end of sequences of the target tokens\n","    sos_idx = tokenizer_tgt.token_to_id('[SOS]')\n","    eos_idx = tokenizer_tgt.token_to_id('[EOS]')\n","\n","    # Computing the output of the encoder for the source sequence\n","    encoder_output = model.encode(source, source_mask)\n","    # Initializing the decoder input with the Start of Sentence token\n","    decoder_input = torch.empty(1,1).fill_(sos_idx).type_as(source).to(device)\n","\n","    # Looping until the 'max_len', maximum length, is reached\n","    while True:\n","        if decoder_input.size(1) == max_len:\n","            break\n","\n","        # Building a mask for the decoder input\n","        decoder_mask = casual_mask(decoder_input.size(1)).type_as(source_mask).to(device)\n","\n","        # Calculating the output of the decoder\n","        out = model.decode(encoder_output, source_mask, decoder_input, decoder_mask)\n","\n","        # Applying the projection layer to get the probabilities for the next token\n","        prob = model.project(out[:, -1])\n","\n","        # Selecting token with the highest probability\n","        _, next_word = torch.max(prob, dim=1)\n","        decoder_input = torch.cat([decoder_input, torch.empty(1,1). type_as(source).fill_(next_word.item()).to(device)], dim=1)\n","\n","        # If the next token is an End of Sentence token, we finish the loop\n","        if next_word == eos_idx:\n","            break\n","\n","    return decoder_input.squeeze(0) # Sequence of tokens generated by the decoder"],"metadata":{"id":"ybJAWeP1eA_M","executionInfo":{"status":"ok","timestamp":1720193065228,"user_tz":-420,"elapsed":1,"user":{"displayName":"Tuấn Nguyễn Duy","userId":"13781921621114420072"}}},"execution_count":59,"outputs":[]},{"cell_type":"code","source":["# Defining function to evaluate the model on the validation dataset\n","# num_examples = 2, two examples per run\n","def run_validation(model, validation_ds, tokenizer_src, tokenizer_tgt, max_len, device, print_msg, global_state, writer, num_examples=2):\n","    model.eval() # Setting model to evaluation mode\n","    count = 0 # Initializing counter to keep track of how many examples have been processed\n","\n","    console_width = 80 # Fixed witdh for printed messages\n","\n","    # Creating evaluation loop\n","    with torch.no_grad(): # Ensuring that no gradients are computed during this process\n","        for batch in validation_ds:\n","            count += 1\n","            encoder_input = batch['encoder_input'].to(device)\n","            encoder_mask = batch['encoder_mask'].to(device)\n","\n","            # Ensuring that the batch_size of the validation set is 1\n","            assert encoder_input.size(0) ==  1, 'Batch size must be 1 for validation.'\n","\n","            # Applying the 'greedy_decode' function to get the model's output for the source text of the input batch\n","            model_out = greedy_decode(model, encoder_input, encoder_mask, tokenizer_src, tokenizer_tgt, max_len, device)\n","\n","            # Retrieving source and target texts from the batch\n","            source_text = batch['src_text'][0]\n","            target_text = batch['tgt_text'][0] # True translation\n","            model_out_text = tokenizer_tgt.decode(model_out.detach().cpu().numpy()) # Decoded, human-readable model output\n","\n","            # Printing results\n","            print_msg('-'*console_width)\n","            print_msg(f'SOURCE: {source_text}')\n","            print_msg(f'TARGET: {target_text}')\n","            print_msg(f'PREDICTED: {model_out_text}')\n","\n","            # After two examples, we break the loop\n","            if count == num_examples:\n","                break"],"metadata":{"id":"8Ct3yoQ8eKgR","executionInfo":{"status":"ok","timestamp":1720193065228,"user_tz":-420,"elapsed":1,"user":{"displayName":"Tuấn Nguyễn Duy","userId":"13781921621114420072"}}},"execution_count":60,"outputs":[]},{"cell_type":"markdown","source":["**Training Loop**"],"metadata":{"id":"cVm3xcIyeO9V"}},{"cell_type":"code","source":["# We pass as parameters the config dictionary, the length of the vocabylary of the source language and the target language\n","def get_model(config, vocab_src_len, vocab_tgt_len):\n","\n","    # Loading model using the 'build_transformer' function.\n","    # We will use the lengths of the source language and target language vocabularies, the 'seq_len', and the dimensionality of the embeddings\n","    model = build_transformer(vocab_src_len, vocab_tgt_len, config['seq_len'], config['seq_len'], config['d_model'])\n","    return model"],"metadata":{"id":"Q2Z44p1DeMyJ","executionInfo":{"status":"ok","timestamp":1720193067014,"user_tz":-420,"elapsed":789,"user":{"displayName":"Tuấn Nguyễn Duy","userId":"13781921621114420072"}}},"execution_count":61,"outputs":[]},{"cell_type":"code","source":["# Define settings for building and training the transformer model\n","def get_config():\n","    return{\n","        'batch_size': 8,\n","        'num_epochs': 20,\n","        'lr': 10**-4,\n","        'seq_len': 350,\n","        'd_model': 512, # Dimensions of the embeddings in the Transformer. 512 like in the \"Attention Is All You Need\" paper.\n","        'lang_src': 'en',\n","        'lang_tgt': 'it',\n","        'model_folder': 'weights',\n","        'model_basename': 'tmodel_',\n","        'preload': None,\n","        'tokenizer_file': 'tokenizer_{0}.json',\n","        'experiment_name': 'runs/tmodel'\n","    }\n","\n","\n","# Function to construct the path for saving and retrieving model weights\n","def get_weights_file_path(config, epoch: str):\n","    model_folder = config['model_folder'] # Extracting model folder from the config\n","    model_basename = config['model_basename'] # Extracting the base name for model files\n","    model_filename = f\"{model_basename}{epoch}.pt\" # Building filename\n","    return str(Path('.')/ model_folder/ model_filename) # Combining current directory, the model folder, and the model filename"],"metadata":{"id":"ZaSgPMwOeRHJ","executionInfo":{"status":"ok","timestamp":1720193067014,"user_tz":-420,"elapsed":2,"user":{"displayName":"Tuấn Nguyễn Duy","userId":"13781921621114420072"}}},"execution_count":62,"outputs":[]},{"cell_type":"code","source":["def train_model(config):\n","    # Setting up device to run on GPU to train faster\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","    print(f\"Using device {device}\")\n","\n","    # Creating model directory to store weights\n","    Path(config['model_folder']).mkdir(parents=True, exist_ok=True)\n","\n","    # Retrieving dataloaders and tokenizers for source and target languages using the 'get_ds' function\n","    train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt = get_ds(config)\n","\n","    # Initializing model on the GPU using the 'get_model' function\n","    model = get_model(config,tokenizer_src.get_vocab_size(), tokenizer_tgt.get_vocab_size()).to(device)\n","\n","    # Tensorboard\n","    writer = SummaryWriter(config['experiment_name'])\n","\n","    # Setting up the Adam optimizer with the specified learning rate from the '\n","    # config' dictionary plus an epsilon value\n","    optimizer = torch.optim.Adam(model.parameters(), lr=config['lr'], eps = 1e-9)\n","\n","    # Initializing epoch and global step variables\n","    initial_epoch = 0\n","    global_step = 0\n","\n","    # Checking if there is a pre-trained model to load\n","    # If true, loads it\n","    if config['preload']:\n","        model_filename = get_weights_file_path(config, config['preload'])\n","        print(f'Preloading model {model_filename}')\n","        state = torch.load(model_filename) # Loading model\n","\n","        # Sets epoch to the saved in the state plus one, to resume from where it stopped\n","        initial_epoch = state['epoch'] + 1\n","        # Loading the optimizer state from the saved model\n","        optimizer.load_state_dict(state['optimizer_state_dict'])\n","        # Loading the global step state from the saved model\n","        global_step = state['global_step']\n","\n","    # Initializing CrossEntropyLoss function for training\n","    # We ignore padding tokens when computing loss, as they are not relevant for the learning process\n","    # We also apply label_smoothing to prevent overfitting\n","    loss_fn = nn.CrossEntropyLoss(ignore_index = tokenizer_src.token_to_id('[PAD]'), label_smoothing = 0.1).to(device)\n","\n","    # Initializing training loop\n","\n","    # Iterating over each epoch from the 'initial_epoch' variable up to\n","    # the number of epochs informed in the config\n","    for epoch in range(initial_epoch, config['num_epochs']):\n","\n","        # Initializing an iterator over the training dataloader\n","        # We also use tqdm to display a progress bar\n","        batch_iterator = tqdm(train_dataloader, desc = f'Processing epoch {epoch:02d}')\n","\n","        # For each batch...\n","        for batch in batch_iterator:\n","            model.train() # Train the model\n","\n","            # Loading input data and masks onto the GPU\n","            encoder_input = batch['encoder_input'].to(device)\n","            decoder_input = batch['decoder_input'].to(device)\n","            encoder_mask = batch['encoder_mask'].to(device)\n","            decoder_mask = batch['decoder_mask'].to(device)\n","\n","            # Running tensors through the Transformer\n","            encoder_output = model.encode(encoder_input, encoder_mask)\n","            decoder_output = model.decode(encoder_output, encoder_mask, decoder_input, decoder_mask)\n","            proj_output = model.project(decoder_output)\n","\n","            # Loading the target labels onto the GPU\n","            label = batch['label'].to(device)\n","\n","            # Computing loss between model's output and true labels\n","            loss = loss_fn(proj_output.view(-1, tokenizer_tgt.get_vocab_size()), label.view(-1))\n","\n","            # Updating progress bar\n","            batch_iterator.set_postfix({f\"loss\": f\"{loss.item():6.3f}\"})\n","\n","            writer.add_scalar('train loss', loss.item(), global_step)\n","            writer.flush()\n","\n","            # Performing backpropagation\n","            loss.backward()\n","\n","            # Updating parameters based on the gradients\n","            optimizer.step()\n","\n","            # Clearing the gradients to prepare for the next batch\n","            optimizer.zero_grad()\n","\n","            global_step += 1 # Updating global step count\n","\n","        # We run the 'run_validation' function at the end of each epoch\n","        # to evaluate model performance\n","        run_validation(model, val_dataloader, tokenizer_src, tokenizer_tgt, config['seq_len'], device, lambda msg: batch_iterator.write(msg), global_step, writer)\n","\n","        # Saving model\n","        model_filename = get_weights_file_path(config, f'{epoch:02d}')\n","        # Writting current model state to the 'model_filename'\n","        torch.save({\n","            'epoch': epoch, # Current epoch\n","            'model_state_dict': model.state_dict(),# Current model state\n","            'optimizer_state_dict': optimizer.state_dict(), # Current optimizer state\n","            'global_step': global_step # Current global step\n","        }, model_filename)\n"],"metadata":{"id":"6sCwdxRSeZGX","executionInfo":{"status":"ok","timestamp":1720193067014,"user_tz":-420,"elapsed":2,"user":{"displayName":"Tuấn Nguyễn Duy","userId":"13781921621114420072"}}},"execution_count":63,"outputs":[]},{"cell_type":"code","source":["if __name__ == '__main__':\n","    warnings.filterwarnings('ignore') # Filtering warnings\n","    config = get_config() # Retrieving config settings\n","    train_model(config) # Training model with the config arguments"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NAYWzQ7Kej-y","executionInfo":{"status":"ok","timestamp":1720201315844,"user_tz":-420,"elapsed":8248832,"user":{"displayName":"Tuấn Nguyễn Duy","userId":"13781921621114420072"}},"outputId":"39613a9a-b8a2-4ed0-de77-cd4e4229f070"},"execution_count":64,"outputs":[{"output_type":"stream","name":"stdout","text":["Using device cuda\n","Max length of source sentence: 309\n","Max length of target sentence: 274\n"]},{"output_type":"stream","name":"stderr","text":["Processing epoch 00: 100%|██████████| 3638/3638 [06:51<00:00,  8.85it/s, loss=5.808]\n"]},{"output_type":"stream","name":"stdout","text":["--------------------------------------------------------------------------------\n","SOURCE: This put me in mind of the first time when I came on shore, and began to look about me; how I gave myself over for lost; how wildly I looked round me; what dreadful apprehensions I had; and how I lodged in the tree all night for fear of being devoured by wild beasts.\n","TARGET: Ciò ricordavami il primo istante del mio naufragio su questo lido: onde cominciai a considerare sopra me stesso; a ricordarmi come anch’io mi fossi dato per perduto; come girava gli occhi stralunati all’intorno; quali tremende paure m’incalzarono; come quella di essere divorato dalle fiere mi fe’ scegliere a stanza un albero per tutta una notte. Que’ poveri sfortunati, io pensava, sono nel mio caso d’allora.\n","PREDICTED: Non mi , e mi , ma mi , e mi , e mi , e mi , e non mi , e non mi , e non mi , e non mi , e non mi , e non mi , e non mi .\n","--------------------------------------------------------------------------------\n","SOURCE: Since their talk about religion during their engagement neither he nor she had ever started a conversation on that subject; but she continued to observe the rites, went to church, and prayed, always with the same quiet conviction that it was necessary to do so.\n","TARGET: Dopo il loro colloquio sulla religione, quando erano ancora fidanzati, né lui né lei ne avevano mai più parlato; ma lei osservava le sue pratiche, frequentava la chiesa e pregava sempre con la tranquilla costante consapevolezza che così bisognasse fare.\n","PREDICTED: Il signor Rochester era stata stata in un ’ altra , ma che non si era stato in un momento di casa , ma non si , ma non si , e non si , e non si , e non si .\n"]},{"output_type":"stream","name":"stderr","text":["Processing epoch 01: 100%|██████████| 3638/3638 [06:49<00:00,  8.89it/s, loss=5.219]\n"]},{"output_type":"stream","name":"stdout","text":["--------------------------------------------------------------------------------\n","SOURCE: Before going to Moscow, she – being an adept at dressing on comparatively little money – left three dresses to be altered.\n","TARGET: Prima della sua partenza per Mosca ella, che in genere era abilissima nel vestirsi senza spendere eccessivamente, aveva dato a rimodernare tre abiti alla sarta.\n","PREDICTED: È un ’ altra cosa , che si , si , si , , , , la barca .\n","--------------------------------------------------------------------------------\n","SOURCE: \"To the finest fibre of my nature, sir.\"\n","TARGET: — Sì, tanto.\n","PREDICTED: — È un ' altra cosa , signore .\n"]},{"output_type":"stream","name":"stderr","text":["Processing epoch 02: 100%|██████████| 3638/3638 [06:50<00:00,  8.87it/s, loss=4.743]\n"]},{"output_type":"stream","name":"stdout","text":["--------------------------------------------------------------------------------\n","SOURCE: I was soon dressed; and when I heard Mr. Rochester quit Mrs. Fairfax's parlour, I hurried down to it.\n","TARGET: Mi vestii in un attimo, e, quando sentii uscire il signor Rochester dal salotto della vedova, corsi giù.\n","PREDICTED: Mi alzai e mi sedei , e mi parve che la signora Reed .\n","--------------------------------------------------------------------------------\n","SOURCE: Being the third son of the family and not bred to any trade, my head began to be filled very early with rambling thoughts.\n","TARGET: Terzo della famiglia, nè educato ad alcuna professione, la mia testa cominciò sino di buon’ora ad empirsi d’idee fantastiche e vaghe.\n","PREDICTED: Il primo giorno la moglie e la vita non mi , e la mia vita , che mi con la sua vita .\n"]},{"output_type":"stream","name":"stderr","text":["Processing epoch 03: 100%|██████████| 3638/3638 [06:49<00:00,  8.88it/s, loss=5.366]\n"]},{"output_type":"stream","name":"stdout","text":["--------------------------------------------------------------------------------\n","SOURCE: I'll go and wash them.\n","TARGET: Andrò a lavarmi.\n","PREDICTED: Io vado via e .\n","--------------------------------------------------------------------------------\n","SOURCE: He called on me and I liked him very much,' she added, with obvious ill intent. 'Where is he?'\n","TARGET: È stato da me, e m’è piaciuto molto — soggiunse con un’evidente intenzione perversa. — Dov’è?\n","PREDICTED: Egli mi ha detto di lui e mi ha detto : “ ” è così semplice ” è impossibile ?”\n"]},{"output_type":"stream","name":"stderr","text":["Processing epoch 04: 100%|██████████| 3638/3638 [06:49<00:00,  8.88it/s, loss=4.723]\n"]},{"output_type":"stream","name":"stdout","text":["--------------------------------------------------------------------------------\n","SOURCE: In the cathedral Levin, with the others, raised his hand, repeating the words of the priest, and swore by the most awful oaths to fulfil all the things the Governor had hoped for.\n","TARGET: Nella cattedrale Levin, sollevando il braccio e ripetendo le parole dell’arciprete insieme con gli altri, giurò con i giuramenti più terribili di compiere tutto quello che sperava il governatore.\n","PREDICTED: In quel momento , Levin , con le sue mani , il suo sorriso , la sua voce , la sua abitudine , e la sua abitudine aveva dato a tutti i .\n","--------------------------------------------------------------------------------\n","SOURCE: Then they knocked up a little place for him at the bottom of the garden, about quarter of a mile from the house, and made him take the machine down there when he wanted to work it; and sometimes a visitor would come to the house who knew nothing of the matter, and they would forget to tell him all about it, and caution him, and he would go out for a stroll round the garden and suddenly get within earshot of those bagpipes, without being prepared for it, or knowing what it was.\n","TARGET: Poi gli riservarono un posticino nel fondo del giardino, a circa un quarto di miglio dall’abitazione, e gli facevan portar giù lo strumento, quando aveva bisogno di farlo lavorare; e a volte qualche visitatore ignaro della faccenda, che dimenticavano di avvertire e mettere in guardia, si trovava — facendo una passeggiatina, in giardino — improvvisamente a tiro della cornamusa, senza esser preparato e senza sapere che fosse.\n","PREDICTED: Poi si misero a un miglio per un miglio per un miglio , e poi , dopo aver fatto un miglio , e poi , quando egli si sarebbe venuto a fare un tratto , e che , per un tratto , si sarebbe venuto a fare un tratto , e che non si , e che si , e si , e si , e si , e si a un tratto , e si , e si a fare un tratto , e si a un tratto a fare , senza che si a fare un tratto a fare un tratto , e si .\n"]},{"output_type":"stream","name":"stderr","text":["Processing epoch 05: 100%|██████████| 3638/3638 [06:49<00:00,  8.88it/s, loss=4.795]\n"]},{"output_type":"stream","name":"stdout","text":["--------------------------------------------------------------------------------\n","SOURCE: \"Tyutkin, Coiffeur.\"...\n","TARGET: E mi odia.\n","PREDICTED: — No , è vero .\n","--------------------------------------------------------------------------------\n","SOURCE: Once he dined there, and the other time he spent an evening with some visitors, but he had not once stayed the night, as he used to do in former years.\n","TARGET: Una volta vi aveva pranzato, un’altra volta aveva passato la serata con ospiti, ma non vi aveva neanche una volta passato la notte, come era solito fare gli anni precedenti.\n","PREDICTED: Una volta , una volta , e l ’ altra sera , l ’ altra sera , l ’ altra sera , non c ’ era più che l ’ anno prima , ma l ’ anno prima era già stato già più di prima .\n"]},{"output_type":"stream","name":"stderr","text":["Processing epoch 06: 100%|██████████| 3638/3638 [06:49<00:00,  8.88it/s, loss=3.889]\n"]},{"output_type":"stream","name":"stdout","text":["--------------------------------------------------------------------------------\n","SOURCE: The same lady pays for the education and clothing of an orphan from the workhouse, on condition that she shall aid the mistress in such menial offices connected with her own house and the school as her occupation of teaching will prevent her having time to discharge in person.\n","TARGET: \"La stessa signora paga per l'educazione e per il vestiario di un'orfana della manifattura a condizione che la ragazza aiuti la maestra nel servizio di casa e di scuola.\n","PREDICTED: La signora Stahl per la cultura di una donna di donna , che la donna , che la signora Pierrot , in Russia , e la sua scienza in Russia , come se ne la sua scuola e la sua presenza .\n","--------------------------------------------------------------------------------\n","SOURCE: 'Why not?\n","TARGET: — Come mai?\n","PREDICTED: — Perché ?\n"]},{"output_type":"stream","name":"stderr","text":["Processing epoch 07: 100%|██████████| 3638/3638 [06:50<00:00,  8.87it/s, loss=3.436]\n"]},{"output_type":"stream","name":"stdout","text":["--------------------------------------------------------------------------------\n","SOURCE: The Mulhausen system – that is already a fact. I expect you know about it.'\n","TARGET: L’organizzazione di Mühlhausen è già un fatto, forse lo sapete.\n","PREDICTED: L ’ affare dell ’ affare è già già già già già da parte .\n","--------------------------------------------------------------------------------\n","SOURCE: Poor child!--poor girl!\"\n","TARGET: Povera piccina! Povera ragazza!\n","PREDICTED: Che bimba !\n"]},{"output_type":"stream","name":"stderr","text":["Processing epoch 08: 100%|██████████| 3638/3638 [06:50<00:00,  8.86it/s, loss=2.743]\n"]},{"output_type":"stream","name":"stdout","text":["--------------------------------------------------------------------------------\n","SOURCE: The Princess could only sob in reply.\n","TARGET: Le risposero i singhiozzi della principessa.\n","PREDICTED: La principessa si poteva dire solo il discorso .\n","--------------------------------------------------------------------------------\n","SOURCE: He strayed down a walk edged with box, with apple trees, pear trees, and cherry trees on one side, and a border on the other full of all sorts of old-fashioned flowers, stocks, sweet-williams, primroses, pansies, mingled with southernwood, sweet-briar, and various fragrant herbs. They were fresh now as a succession of April showers and gleams, followed by a lovely spring morning, could make them: the sun was just entering the dappled east, and his light illumined the wreathed and dewy orchard trees and shone down the quiet walks under them.\n","TARGET: Egli camminava in un sentiero limitato da una siepe di bosso; da un lato si vedevano i peri, i meli e i ciliegi, dall'altro i garofani, il biancospino e le erbe odorifere; erano così belle ristorate dal sole e dalle frequenti pioggie; il sole faceva brillare la rugiada sulle foglie e mandava i suoi raggi nel sentiero solitario ove eravamo.\n","PREDICTED: il ghiaccio con le cime coperte di neve e di , e un ’ ombra di rose , e tutto si di tutte le altre file di fiori , e di fiori , di e di fiori , di e di , di e di , in un ’ atmosfera che il sole , si stendeva sotto la luce e le pareti , e le colline , le ombre , e le ali .\n"]},{"output_type":"stream","name":"stderr","text":["Processing epoch 09: 100%|██████████| 3638/3638 [06:50<00:00,  8.85it/s, loss=3.831]\n"]},{"output_type":"stream","name":"stdout","text":["--------------------------------------------------------------------------------\n","SOURCE: His sisters were gone to Morton in my stead: I sat reading Schiller; he, deciphering his crabbed Oriental scrolls.\n","TARGET: Le sue sorelle andarono a Morton in vece mia ed io ero intenta a leggere Schiller, mentre egli studiava la sua lingua orientale.\n","PREDICTED: Le sorelle erano occupate a Morton ; egli mi lesse il suo piano e il procuratore sui suoi consigli .\n","--------------------------------------------------------------------------------\n","SOURCE: Can I get to the marsh this way?'\n","TARGET: Si va di qua alla palude?\n","PREDICTED: Posso lasciare la palude ?\n"]},{"output_type":"stream","name":"stderr","text":["Processing epoch 10: 100%|██████████| 3638/3638 [06:50<00:00,  8.87it/s, loss=3.845]\n"]},{"output_type":"stream","name":"stdout","text":["--------------------------------------------------------------------------------\n","SOURCE: \"How long will you stay?\"\n","TARGET: — Quanto volete rimanere assente?\n","PREDICTED: — Come a lungo ?\n","--------------------------------------------------------------------------------\n","SOURCE: \"Let it be got ready instantly; and if your post-boy can drive me to Ferndean before dark this day, I'll pay both you and him twice the hire you usually demand.\"\n","TARGET: — Fatelo attaccare e dite al vetturino che se può condurmi prima di notte a Ferndean, pagherò a lui e a voi il doppio di quel che pagano al solito.\n","PREDICTED: — Signore ! — e , se il vostro ragazzo può venire a scuola , il giorno prima , vi darò due o tre giorni prima che lo .\n"]},{"output_type":"stream","name":"stderr","text":["Processing epoch 11: 100%|██████████| 3638/3638 [06:49<00:00,  8.88it/s, loss=2.383]\n"]},{"output_type":"stream","name":"stdout","text":["--------------------------------------------------------------------------------\n","SOURCE: She put down her glasses and was about to go; but at that moment an officer galloped up and reported something to the Emperor.\n","TARGET: Abbassò il binocolo e fece per andarsene; ma in quel momento giunse un ufficiale a cavallo a riferire qualcosa allo zar.\n","PREDICTED: Ella abbracciò e si spense per andare a prendere un attimo ; ma , a un tratto , l ’ ufficiale , pregò di fare qualcosa .\n","--------------------------------------------------------------------------------\n","SOURCE: She recalled the ball and Vronsky and his humble, enamoured gaze, and their relations with one another; there was nothing to be ashamed of.\n","TARGET: Ricordò il ballo, ricordò Vronskij e il suo viso innamorato, sottomesso, ricordò tutti i suoi rapporti con lui; non c’era nulla di cui vergognarsi.\n","PREDICTED: Le persone che e Vronskij e la sua , l ’ aspetto di amicizia e di amicizia con i loro rapporti , erano state già da vergognarsi .\n"]},{"output_type":"stream","name":"stderr","text":["Processing epoch 12: 100%|██████████| 3638/3638 [06:49<00:00,  8.88it/s, loss=3.178]\n"]},{"output_type":"stream","name":"stdout","text":["--------------------------------------------------------------------------------\n","SOURCE: \"Do you, sir, feel calm and happy?\"\n","TARGET: — Signore, vi sentite calmo e felice?\n","PREDICTED: — Volete , signore , calmo e calma .\n","--------------------------------------------------------------------------------\n","SOURCE: Had they thought fit to have gone to sleep there, as the other part of them had done, they had done the job for us; but they were too full of apprehensions of danger to venture to go to sleep, though they could not tell what the danger was they had to fear.\n","TARGET: Se avessero stimato bene di condursi a quell’ombra per dormire, come avea fatto la prima banda, ci avrebbero reso un bel servigio, ma troppo erano pieni di paura per avventurarsi a dormire, ancorchè finora non sapessero qual fosse il pericolo che dovevano temere.\n","PREDICTED: Se avessero potuto far di tempo a dormire , come se ne rimanevano tutti gli altri , avevano fatti il lavoro per ; ma non poterono farlo pensare a se stessi lupi , per conseguenza di ogni pericolo di essere in pericolo , pur rimanendo in pericolo a morire .\n"]},{"output_type":"stream","name":"stderr","text":["Processing epoch 13: 100%|██████████| 3638/3638 [06:49<00:00,  8.88it/s, loss=2.644]\n"]},{"output_type":"stream","name":"stdout","text":["--------------------------------------------------------------------------------\n","SOURCE: Would you believe it, I am just like a starving woman to whom a full meal has been served, and who does not know what to begin on first?\n","TARGET: Tu non ci crederai, ma io sono proprio come un’affamata alla quale, d’un tratto, abbiano messo dinanzi un intero pranzo, e che non sa da che cosa cominciare.\n","PREDICTED: Volete che sia una donna come una fino a quel pranzo , che è stato in modo , e che non ne conosce il primo passo ?\n","--------------------------------------------------------------------------------\n","SOURCE: But why should I make you wretched?\n","TARGET: Ma tu perché?\n","PREDICTED: Ma perché soffrire ?\n"]},{"output_type":"stream","name":"stderr","text":["Processing epoch 14: 100%|██████████| 3638/3638 [06:49<00:00,  8.88it/s, loss=2.505]\n"]},{"output_type":"stream","name":"stdout","text":["--------------------------------------------------------------------------------\n","SOURCE: There was nothing for him to exercise his penetration upon this time.\n","TARGET: Non aveva su che cosa esercitare la propria penetrazione.\n","PREDICTED: Non v ' era nulla di più per lui il suo tempo di farsi pensare alla sua speranza .\n","--------------------------------------------------------------------------------\n","SOURCE: 'Then it ought to be Number One,' said Alice.\n","TARGET: — Allora dovrebbe essere la prima, — disse Alice.\n","PREDICTED: — Allora dovrebbe essere , — disse Alice .\n"]},{"output_type":"stream","name":"stderr","text":["Processing epoch 15: 100%|██████████| 3638/3638 [06:49<00:00,  8.87it/s, loss=2.494]\n"]},{"output_type":"stream","name":"stdout","text":["--------------------------------------------------------------------------------\n","SOURCE: 'Not quite right, I'm afraid,' said Alice, timidly; 'some of the words have got altered.'\n","TARGET: — Temo di no, — rispose timidamente Alice, — certo alcune parole sono diverse.\n","PREDICTED: — Niente affatto , certamente , — rispose Alice , — ma tante parole ! — È un ' altra delle parole .\n","--------------------------------------------------------------------------------\n","SOURCE: 'Frogs or no frogs... I don't publish a newspaper and don't want to defend them, but I am speaking of the unanimity of the intelligent world,' said Koznyshev, turning to his brother.\n","TARGET: — Ranocchie o non ranocchie, io giornali non ne pubblico e non li voglio difendere; ma parlo dell’unità di pensiero nel mondo dell’intelligencija — disse Sergej Ivanovic, rivolto al fratello.\n","PREDICTED: — o non conosco . Non capisco che la mia abilità abbia il tedesco per il suo libro e non mi voglia di , ma di un tratto — disse Sergej Ivanovic , rivolgendosi al fratello .\n"]},{"output_type":"stream","name":"stderr","text":["Processing epoch 16: 100%|██████████| 3638/3638 [06:50<00:00,  8.87it/s, loss=2.698]\n"]},{"output_type":"stream","name":"stdout","text":["--------------------------------------------------------------------------------\n","SOURCE: \"It is fair to-night,\" said she, as she looked through the panes, \"though not starlight; Mr. Rochester has, on the whole, had a favourable day for his journey.\"\n","TARGET: — Non ci sono stelle, — disse, — ma è tempo buono.\n","PREDICTED: — È bella stasera , — disse , guardando il ghiaccio , senza nubi , che non aveva il coraggio . Il giorno è felice .\n","--------------------------------------------------------------------------------\n","SOURCE: 'Fine!\n","TARGET: — Ottimo!\n","PREDICTED: — , .\n"]},{"output_type":"stream","name":"stderr","text":["Processing epoch 17: 100%|██████████| 3638/3638 [06:50<00:00,  8.86it/s, loss=2.109]\n"]},{"output_type":"stream","name":"stdout","text":["--------------------------------------------------------------------------------\n","SOURCE: \"What is the employment you had in view, Mr. Rivers?\n","TARGET: — Qual'era l'occupazione che avevate in vista, signor Rivers?\n","PREDICTED: — Quale è stata l ' occupazione di vostro signor Rivers ?\n","--------------------------------------------------------------------------------\n","SOURCE: They already prepared lumps of mud to pelt her with in due time.\n","TARGET: Preparavano già il fango da scagliare su di lei, non appena fosse giunto il momento.\n","PREDICTED: Si già a per di fango .\n"]},{"output_type":"stream","name":"stderr","text":["Processing epoch 18: 100%|██████████| 3638/3638 [06:49<00:00,  8.88it/s, loss=2.262]\n"]},{"output_type":"stream","name":"stdout","text":["--------------------------------------------------------------------------------\n","SOURCE: But Kitty was in one of her fits of passion.\n","TARGET: Ma Kitty era tutta presa dall’ira.\n","PREDICTED: Kitty era in lei un ’ indole di collera .\n","--------------------------------------------------------------------------------\n","SOURCE: I don't understand.\"\n","TARGET: Non capisco.\n","PREDICTED: Non capisco .\n"]},{"output_type":"stream","name":"stderr","text":["Processing epoch 19: 100%|██████████| 3638/3638 [06:49<00:00,  8.87it/s, loss=1.873]\n"]},{"output_type":"stream","name":"stdout","text":["--------------------------------------------------------------------------------\n","SOURCE: A negotiation was opened through the medium of the ambassador, Sam; and after much pacing to and fro, till, I think, the said Sam's calves must have ached with the exercise, permission was at last, with great difficulty, extorted from the rigorous Sibyl, for the three to wait upon her in a body.\n","TARGET: Allora furono aperte le trattative con l'intermediario di Sam, e questi andò e tornò tante volte per portare e riferire imbasciate, che doveva avere le gambe rotte. Finalmente, dopo molte trattative, la rigorosa sibilla permise alle tre ragazze di andare insieme.\n","PREDICTED: La sala era aperta dello specchio , il comando a camminare , e a far moto la coda , disse : — Forse non è occupata , signore , perché il compito era duro , gran piacere di trovare la difficoltà del compito , e disse che stava per apparire una tempesta .\n","--------------------------------------------------------------------------------\n","SOURCE: 'Yes, I am very much interested in it,' said Anna to Sviyazhsky, who expressed surprise at her knowledge of architecture. 'The new building ought to be in line with the hospital, but it was an afterthought and was begun without a plan.'\n","TARGET: — Sì, me ne interesso molto — rispose Anna a Svijazskij, che aveva espresso meraviglia per le sue nozioni di architettura. — Bisogna che la nuova costruzione corrisponda all’ospedale. Ma è stata concepita dopo e cominciata senza progetto.\n","PREDICTED: — Sì , molto a questo punto le interessa molto — disse Anna , mostrando con meraviglia la conoscenza degli alti malati , che si potevano avere in ombra di cortesia a quell ’ ospedale . — C ’ era un ’ ospedale e perdere il piano , e cominciò a spiegare il termine .\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"psAaKP4CfTZw"},"execution_count":null,"outputs":[]}]}